{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using invariant representations to guide future exploration\n",
    "\n",
    "I would like to further explore an idea of leveraging abstract concepts learned from previous experiences to help aid and guide the behavior of an agent when faced with a novel problem. The aim is to sequentially construct and expand agent's knowledge base, and use it to construct a behavioral policy that would guide the exploration while learning the optimal policy.\n",
    "\n",
    "## Implementation Plan\n",
    "\n",
    "The following is a rough outline of the implementation plan:\n",
    "- [ ] Allow training of agents via Q-Learning for wide range of tasks. At this stage, the goal is to setup the training framework, logging, metrics and everything you might need. In addition, make sure that the behavioral policy of the Q-Learning agent is swappable.\n",
    "- [ ] Create invariant representation of learned experience. For each new task, first sample tuples of experiences, train an autoencoder network and add that particular task to the knowledge base.\n",
    "- [ ] Create behavioral policy for a new task, from the knowledge base. At this stage, we alter the training process to:\n",
    "  - [ ] Let the agent explore for a limited amount of time.\n",
    "  - [ ] Determine a set of most relevant tasks.\n",
    "  - [ ] Construct a behavior policy from a set of most relevant tasks and integrate it into Q-Learning.\n",
    "- [ ] Lastly, I perform the experiment phase. Here, a carefully selected set of tasks  needs to be run, both individually as a baseline, and then in sequence using the knowledge base. If time permits, running several experiments with differently ordered tasks could be useful to obtain standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if os.path.abspath(os.path.join('.')) not in sys.path:\n",
    "    sys.path.append(os.path.abspath(os.path.join('.')))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Task, LLDQNTrainer, KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = KnowledgeBase()\n",
    "task = Task(env_name=\"CartPole-v1\", knowledge_base=knowledge_base)\n",
    "trainer = LLDQNTrainer(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  70%|######9   | 6990/10000 [00:16<00:07, 426.91it/s, env_step=6990, len=154, loss=0.286, n/ep=0, n/st=10, rew=154.00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aleksa/Development/Source/university/lifelong_learning_project/src/main.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aleksa/Development/Source/university/lifelong_learning_project/src/main.ipynb#ch0000004?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/Development/Source/university/lifelong_learning_project/src/models/trainer.py:27\u001b[0m, in \u001b[0;36mLLDQNTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/tianshou/trainer/base.py:418\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[1;32m    420\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[1;32m    421\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/tianshou/trainer/base.py:270\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m result: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     data, result, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[1;32m    271\u001b[0m     t\u001b[39m.\u001b[39mupdate(result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/tianshou/trainer/base.py:361\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_fn:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_step)\n\u001b[0;32m--> 361\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_collector\u001b[39m.\u001b[39;49mcollect(\n\u001b[1;32m    362\u001b[0m     n_step\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_per_collect, n_episode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisode_per_collect\n\u001b[1;32m    363\u001b[0m )\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mn/ep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_metric:\n\u001b[1;32m    365\u001b[0m     rew \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_metric(result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/tianshou/data/collector.py:236\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m no_grad:\n\u001b[1;32m    234\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# faster than retain_grad version\u001b[39;00m\n\u001b[1;32m    235\u001b[0m         \u001b[39m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, last_state)\n\u001b[1;32m    237\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, last_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/tianshou/policy/modelfree/dqn.py:157\u001b[0m, in \u001b[0;36mDQNPolicy.forward\u001b[0;34m(self, batch, state, model, input, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m obs \u001b[39m=\u001b[39m batch[\u001b[39minput\u001b[39m]\n\u001b[1;32m    156\u001b[0m obs_next \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mobs \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obs, \u001b[39m\"\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m obs\n\u001b[0;32m--> 157\u001b[0m logits, hidden \u001b[39m=\u001b[39m model(obs_next, state\u001b[39m=\u001b[39;49mstate, info\u001b[39m=\u001b[39;49mbatch\u001b[39m.\u001b[39;49minfo)\n\u001b[1;32m    158\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_q_value(logits, \u001b[39mgetattr\u001b[39m(obs, \u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax_action_num\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Development/Source/university/lifelong_learning_project/src/models/network.py:20\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m     17\u001b[0m     obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(obs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n\u001b[1;32m     19\u001b[0m batch \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(obs\u001b[39m.\u001b[39;49mview(batch, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m logits, state\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/envs/lifelong_learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('lifelong_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33431a34c2fc9292c5ffdbeed9358857b48cb59106e4a69bd1d3c3cefbd11921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
